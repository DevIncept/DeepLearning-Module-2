## Introduction to Layers #3

Welcome back :blush:. Trust your yearning for learning is being satisfied :clap:. 

In this final section of introduction to layers, we would discuss on some layers of Keras API.

Trust you have learnt about Keras as one of the powerful open-source frameworks used in Machine Learning and Deep Learning.

Layers are basic building blocks of neural networks in Keras. These layers are built in layers in Keras, and they help to make computations in neural networks simpler. 

There are different categories of layers such as: 
* activation layers
* layer weight installers
* layer weight regularizers
* core layers 
* convolution layers 
* pooling layers 
* recurrent layers 
* pre-processing layers
* normalization layers 
* reshaping layers 
* merging layers
among others

For example, activation layers consist of different functions responsible for the activation of functions in neural networks, of which sigmoid, ReLu and tanh functions are a part; 
convolution layers are responsible for different convolution operations such as Conv1D, Conv2D and Conv3D.

This is not exhaustive. You can read more about Keras APIs by clicking on this link :point_right: [Layers of Keras API](https://keras.io/api/layers/).

See you in the next section where we begin to talk about multi-layer neural networks.
